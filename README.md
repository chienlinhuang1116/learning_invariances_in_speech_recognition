# Learning Invariances In Speech Recognition

### Abstract
In this work I investigated the speech command task developing and analyzing deep learning models. State of the art technology uses CNNs models because of their intrinsic nature of learning correlated representations as is the speech. In particular I develop different CNNs trained on the Google Speech Command Dataset and tested on different scenarios. A main problem on speech recognition is in the differences on pronunciations of words among different people: one way of building an invariant model to variability is to augment the dataset perturbing the input maintaining constant the class label. In this work I study two kind of augmentation: the Vocal Tract Length Perturbation (VTLP) and the Synchronous Overlap and Add (SOLA) that locally perturbs the input in frequency and time respectively. The models trained on augmented data outperforms in accuracy, precision and recall all the models trained in the normal dataset. Also the design of CNNs has impact on learning invariances: the inception CNN layer in fact helps on learning features that are invariant to speech variability using different kind of kernel sizes for convolution. Intuitively this is because of the implicit capability of the model on detecting different speech pattern length in the audio feature
